# aws-data-engineering-project

## ðŸ“‚ Dataset

This project uses a synthetic e-commerce dataset created for learning and demonstration purposes.

### Files
- orders.csv â€“ transactional order data
- customers.json â€“ customer master data
- products.csv â€“ product catalog


# Overview

This project demonstrates a production-grade, cloud-native data engineering platform designed using modern industry tools and best practices.
It supports batch processing, API ingestion, and event-driven pipelines, similar to real-world enterprise data platforms.

The project is built entirely using personal/free-tier compatible cloud services, making it realistic, truthful, and interview-safe.

# Business Use Case

An E-commerce Analytics Platform that enables:

Sales and revenue reporting

Customer behavior analysis

Product performance insights

Near real-time ingestion of new data files

# High-Level Architecture

## Data Ingestion Patterns
## Batch Ingestion (Scheduled)

CSV files uploaded to S3

Triggered daily using Apache Airflow

## API-Based Ingestion

Public REST API data fetched using Python

Stored as raw JSON in S3

## Event-Driven Ingestion

S3 object arrival triggers AWS Lambda

Lambda triggers AWS Glue ETL job

## Technology Stack

Source	: CSV Files, Public REST API,JSON
Storage	: Amazon S3
Event   : Trigger	AWS Lambda
ETL Processing	: AWS Glue (PySpark)
Orchestration	: Apache Airflow
Alerting	: Amazon SNS
Failure Handling	: SQS Dead Letter Queue
Warehouse	: Snowflake
Transformations : dbt
Data Format	: Parquet

## Repository Structure
aws-data-engineering-project/
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ ecommerce_pipeline_dag.py
â”‚
â”œâ”€â”€ glue/
â”‚   â””â”€â”€ ecommerce_glue_job.py
â”‚
â”œâ”€â”€ lambda/
â”‚   â””â”€â”€ s3_trigger_lambda.py
â”‚
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ api_ingestion.py
â”‚
â”œâ”€â”€ snowflake/
â”‚   â”œâ”€â”€ create_tables.sql
â”‚   â””â”€â”€ stage_and_copy.sql
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/
â”‚       â”œâ”€â”€ marts/
â”‚       â”‚   â”œâ”€â”€ dim_customers.sql
â”‚       â”‚   â””â”€â”€ fct_orders.sql
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ customers.csv
â”‚   â””â”€â”€ orders.csv
â”‚
â”œâ”€â”€ diagrams/
â”‚   â””â”€â”€ architecture.png
â”‚
â””â”€â”€ README.md

## End-to-End Pipeline Execution Flow
ðŸ”¹ Step 1: Data Ingestion

CSV files uploaded to S3 (raw/ bucket)

API data fetched via Python script and stored in S3

ðŸ”¹ Step 2: Event Trigger

S3 upload triggers AWS Lambda

Lambda starts AWS Glue ETL job

ðŸ”¹ Step 3: ETL Processing (Glue)

Reads raw data from S3

Applies schema enforcement and cleansing

Writes transformed data to S3 in Parquet format

ðŸ”¹ Step 4: Orchestration (Airflow)

Airflow DAG:

Triggers Glue job (batch mode)

Loads data into Snowflake

Runs dbt transformations

ðŸ”¹ Step 5: Data Warehousing (Snowflake)

External stage reads data from S3

COPY command loads data into raw tables

ðŸ”¹ Step 6: Analytics Engineering (dbt)

Staging models clean raw data

Fact & dimension models created

Incremental loading implemented

SCD Type-2 applied for customer dimension

ðŸ”¹ Step 7: Monitoring & Alerts

Success and failure notifications via SNS

Lambda failures routed to SQS DLQ
